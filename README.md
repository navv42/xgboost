# xgboost
gradient boosting, decision trees, and xgboost from scratch (mostly)
**Building xgboost from scratch using pandas and numpy :)**

implementation will also support user-defined custom objective functions, meaning that it can perform 
regression, classification, and whatever exotic learning tasks you can dream up, as long as you can 
write down a twice-differentiable objective function.

implement column subsampling myself

We are doing the exact tree-splitting algorithm. Other options are sparsity-aware method (used to handle
missing feature values) and the approximate method (used for scalability)


